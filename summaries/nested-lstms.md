# Nested LSTMs

Moniz et. al., Jan 2018

Authors: Joel Ruben Antony Moniz and David Krueger

[[arxiv]](https://arxiv.org/abs/1801.10308) [[other link(s)]](#)

**Tags**: 
- RNNs
- LSTMs

(Work-in-progress)

## Summary

Potential replacement for stacked LSTMs

- Context
	- RNNs have had problems with vanishing gradient -> ameliorated by LSTMs
	- Learning to detect long-term dependencies involves a difficult credit assignment problem
- Memory encodes implicit priors
	- Paper seeks to encode additional implicit prior of **temporaral hierarchy** via nesting
		- Use nesting as an approach to constructing temporal hierarchies in memory
	- **selective access to inner memories** -> frees inner memories to remember and process events on longer time scales

	- 


## Method

{Method}

#### Method details

{Method details}

## Results

{Results}

<!-- Optional sections -->

## Thoughts

## Related papers