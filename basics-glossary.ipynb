{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics Glossary\n",
    "\n",
    "Most of these definitions are paraphrased from [CS231n](http://cs231n.github.io/).\n",
    "\n",
    "### Artificial Neural Networks\n",
    "- 'Perform sequences of linear mappings with interwoven non-linearites'\n",
    "- Typical Components of a Neural Network\n",
    "\t- Input\n",
    "\t- Score function: maps raw data to class scores (for classification)\n",
    "\t\t- Layers\n",
    "\t\t- Output layer\n",
    "\t\t\t- Usually don't have an activation fn\n",
    "\t- Loss Function (also cost function, objective)\n",
    "\t\t- e.g. Softmax, SVM\n",
    "\t\t- to quantify quality of a set of parameters\n",
    "\t- Optimiser (finding parameters to minimise loss)\n",
    "- Size: usually measured by number of parameters\n",
    "\t- Small networks not preferred: they are harder to train with local methods such as gradient descent.\n",
    "\t\t- Many minima of loss functions are bad and easy to converge to -> variance in final loss\n",
    "\t- Large networks: beware of overfitting. \n",
    "\t\t- Tackle using regularisation.\n",
    "- Representational power: NNs with at least one hidden layer are universal approximators\n",
    "\t- but this has little to do with their widespread use\n",
    "- Number of layers\n",
    "\t- (Fully Connected only) In practice, going beyond 3 layers rarely helps much more.\n",
    "\t- CNNs: depth has been found to be extremely important (10+ learnable layers).\n",
    "\n",
    "### Data\n",
    "- Training set\n",
    "\t- Data used to directly train your model\n",
    "- Validation set\n",
    "\t- Data used to tune hyperparameters\n",
    "\t\t- Usually choose hyperparameters with lowest validation error\n",
    "\t- Like a fake test set\n",
    "- Test set\n",
    "\t- Data used to measure generalisation of your model: used only once!\n",
    "- Generalisation\n",
    "- K-fold Cross-validation\n",
    "\t- Iterate over k different validation sets and average performance across them\n",
    "\t- Especially useful if training set is small (e.g. < 1000 exampless).\n",
    "\t- BUT computationally expensive\n",
    "\t- Usually K = 3, 5 or 10\n",
    "\n",
    "### Model performance\n",
    "- Underfit\n",
    "- Overfit\n",
    "- Capacity\n",
    "- Hyperparameters\n",
    "\n",
    "### Things to consider\n",
    "- Computational cost at train and test\n",
    "\n",
    "### Data preprocessing\n",
    "- Centering data by subtracting mean from each feature\n",
    "\t- `X-= np.mean(X, axis=0)`\n",
    "\t- For images, can subtract a single value from all pixels for convenience: `X -= np.mean(X)`.\n",
    "- Normalisation\n",
    "\t- Method (2 ways)\n",
    "\t\t- 1: Divide each zero-centered feature by its standard devation:\n",
    "\t\t\t- `X /= np.std(X, axis=0)`\n",
    "\t\t- 2: Scaling input features so values range from [-1,1]\n",
    "\t- 'Only if different features have different scales but should be of approximately equal importance to the learning algorithm' -> hm\n",
    "- PCA (principle component analysis)\n",
    "\t- Dimensionality reduction: keep K dimensions of data that contain the most variance.\n",
    "\t- Method:\n",
    "\t\t1. Center data at zero (as above).\n",
    "\t\t2. Compute covariance matrix\n",
    "\t\t3. Compute SVD factorisation of data covariance matrix: eigenvectors U, singular values S, V.\n",
    "\t\t4. Project original (zero-centered) data into the eigenbasis\n",
    "\t\t5. Use only the top K eigenvectors (reduce to K dims).\n",
    "- Whitening: \n",
    "\t- Used after PCA. Normalises the scale of the data.\n",
    "\t- Method: Take the data in the eigenbasis and divide each dimension by the egienvalue to normalise the scale\n",
    "\t- Cons: Can exaggerate noise (higher frequencies) since it stretches all dims to be of equal size in the input\n",
    "\t\t- Can mitigate with stronger smoothing\n",
    "- Note that statistics such as the mean should only be computed from training data.\n",
    "- PCA and whitening rarely used with CNNs.\n",
    "\n",
    "### Weight initialisation\n",
    "- Initialise to small random numbers\n",
    "\t- Not too small for deep nets: Smaller weights mean smaller gradients which could reduce the 'gradient signal' flowing backward through a network and so be a problem for deep networks (because there are so many layers it needs to propagate through)\n",
    "- Normalise variance per neuron by 1/sqrt(num_input)\n",
    "\t- Alts:\n",
    "\t\t- For NNs with ReLU neurons: `sqrt(2.0/n)` [(He et. al.)](https://arxiv-web3.library.cornell.edu/abs/1502.01852)\n",
    "\t\t\t- **current recommendation**\n",
    "\t\t- `sqrt(2.0/(n_in + n_out))` [(Glorot et. al.)](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)\n",
    "\t- Variance of dist of outputs from a randomly initialised neuron grows with number of inputs. \n",
    "\t- 1/sqrt(num_inputs) is the recommended heuristic scaling factor \n",
    "\t- So all neurons have approx same output dist, empirically improves rate of convergence\n",
    "\t- Fan-in: number of inputs\n",
    "- Alt: sparse initialisation\n",
    "\t- Set all weight matrices to zero (but say connections sampled from small gaussian? TODO: follow up), but \n",
    "\t- Break symmetry by randomly connecting each neuron to a fixed number of neurons below it, e.g. 10 neurons.\n",
    "- Bias initialisation\n",
    "\t- Commonly initialise to zero\n",
    "\t- (Unclear if provides consistent improvement) ReLU non-linearities: can init to small constant like 0.01 to ensure all ReLU units fire in the begining and thus obtain and propagate some gradient.\n",
    "- Mistakes\n",
    "\t- All-zero initialisation (all weights the same): no asymmetry between neurons, all compute same gradients and undergo same parameter updates. \n",
    "- Batch normalisation \n",
    "\t- Make nets more robust to bad initialisation.\n",
    "\t- Idea: Explicitly forces activations throughout net to take on unit gaussian dist at start of training.\n",
    "\t\t- Possible because normalisation is a simpe differentiable operation.\n",
    "\t\t- Like doing preprocessing at each layer of the net.\n",
    "\t- Method:\n",
    "\t\t- Insert BatchNorm laye immediately after fully connected layers.\n",
    "\t- [Paper by Ioffe and Szegedy, 2015](https://arxiv.org/abs/1502.03167)\n",
    "\n",
    "### Measures of distance\n",
    "\n",
    "Often used for regularisation.\n",
    "\n",
    "- L1 distance \n",
    "\t- $d_1(x, y) = \\sum_i|x_i - y_i|$\n",
    "- L2 distance \n",
    "\t- $d_2(x, y) = \\sqrt{\\sum_i (x_i - y_i)^2}$\n",
    "\n",
    "### Loss Functions (Data loss)\n",
    "- Loss is usually = Data loss + Regularisation loss\n",
    "- Loss usually mean loss per example over training data examples\n",
    "- Multiclass Support Vector Machine (SVM) loss\n",
    "\t- Wants correct class to have score higher than incorrect classes by at least some fixed margin $\\Delta$.\n",
    "\t- $L_i = \\sum_{j\\ne y_i}\\max(0,s_j-(s_{y_i}-\\Delta))$ \n",
    "\t\t- where $s_j$ is the score for the jth class,\n",
    "\t\t- i is the index for examples\n",
    "\t- Aside: can be formulated in other ways (OVA, AVA)\n",
    "\t- Convex function, non-differentiable (due to kinks) but [subgradient](https://en.wikipedia.org/wiki/Subderivative) still exists and is commonly used instead.\n",
    "\t\t- Subgradient: set of slopes of lines drawn through a point f(x0) which is everywhere either touching or below the graph of f.\n",
    "- Cross-entropy loss\n",
    "\t- Minimises cross-entropy between estimated class probabilities and the distribution where all p(x) mass is on the correct class\n",
    "\t- $L_i = -\\log(\\frac{e^{f_{y_i}}{\\sum_j e^{f_j}})$\n",
    "\t\t- equivalently: $L_i = -f_{y_i} + \\log\\sum_j e^{f_j}$\n",
    "\t\t - where $f_j$ = jth element of vector of class scores $f$.\n",
    "\t- Problems:\n",
    "\t\t- If number of classes is very large (e.g. ImageNet 22k classes), may be helpful to use hierarchical softmax\n",
    "\t\t\t- decomposes labels into tree (each label is a path), softmax classifier trained at each node of tree.\n",
    "\t\t\t    - Structure of tree strongly affects performance as is usually problem-dependent\n",
    "\t- More Theory\n",
    "\t\t- Equivalent to minimising Kullback-Leibler divergence $D_{KL}$ between the two distributions. \n",
    "\t\t \t- since $H(p,q) = H(p) + D_{KL}(p||q)$, and $H(p) = 0$.\n",
    "\t\t- Information Theory Cross Entropy\n",
    "\t\t \t- $H(p,q)=-\\sum_xp(x)\\log q(x)$\n",
    "\t\t \t\t- where p is a 'true' distribution and q is an estimated distribution\n",
    "\t- Probabilistic Interpretation\n",
    "\t\t- Interpret $P(y_i|x_i;W) = \\frac{e^{f_{y_i}}{\\sum_j e^{f_j}}$ as the \n",
    "\t\t\t- Normalised probability assigned to the correct label $y_i$ given the image $x_i$ and parameterised by $W$.\n",
    "\t\t\t- since softmax classifier interprets $f$ scores as unnormalised log probabilities.\n",
    "\t\t- i.e. minimising negative log likelihood of the correct class, i.e. performing Maximum Likelihood Estimation (MLE).\n",
    "\t\t- Can thus also interpret R(W) as 'coming from a Gaussian prior over the weight matrix W, where we are performing Maximum a posteriori (MAP) estimation'\n",
    "\t\t- then the cross-entropy loss $L_i = -\\log P(y_i|x_i;W)$.\n",
    "- Softmax function\n",
    " \t- $f_j(z) = \\frac{e^{z_j}{\\sum_k e^{z_k}}$\n",
    " \t- 'Takes vector of arbitrary real-valued scores in z and squashes it to a vector of values between zero and one that sum to one.'\n",
    " \t- Problems: Dividing large numbers (exponentials may be large) may be numerically unstable, so multiply top and bottom by constant C, where e.g. $\\log C = - \\max_j f_j$\n",
    " \t\t- i.e. shift values inside vector $f$ so highest value is zero.\n",
    "- Softmax classifier notes\n",
    "\t- Outputs unnormalised log probabilities whose peakiness depends on regularisation strength.\n",
    "\t\t- Higher regularisation strength -> less peaky\n",
    "- L2 vs Softmax\n",
    "\t- Softmax more stable, L2 harder to optimise\n",
    "- Structured loss\n",
    "\t- Case where labels can be arbitrary structures such as graphs, trees or other complex objects. Space of structured assumed to be large and not easily enumerable.\n",
    "\t- Idea: Deand margin between correct structure y_i and highest-scoring incorrec structure.\n",
    "\t- Usually devise special solvers (as opposed to gradient descent) that exploit simplifying assumptions of the structure space.\n",
    "- For regression\n",
    "\t- L2 norm squared (of the difference between the prediction quantity and true answer)\n",
    "\t\t- i.e. $L_i = ||f-y_i||^2_2$\n",
    "\t\t<!-- TODO: expand this -->\n",
    "\t- L1 norm of the difference between the prediction quantity and true answer\n",
    "\t\t- $L_i = ||f-y_i||_1 = \\sum_j|f_j-(y_i)_j|$\n",
    "\t    - L2 norm squared because gradient becomes \n",
    "\t    simpler_\n",
    "\n",
    "### Regularisation loss\n",
    "- Loss is usually = Data loss + Regularisation loss\n",
    "    - Regularisation loss is usually NOT a function of the data, but of the model parameters\n",
    "- Regularisation: Preference for certain sets of weights over others. \n",
    "\t- Usually to prevent overfitting or reduce ambiguity when there exist multiple solutions (e.g. when weights $\\lambda W$ all yield same output, for positive real $\\lambda$).\n",
    "- L2 norm\n",
    "\t- Elementwise quadratic penalty over all parameters:\n",
    "\t\t- $R(W) = \\sum_k \\sum_l W_{k,l}^2$\n",
    "\t\t\t- sums up all squared elements of W\n",
    "\t\t- often $\\frac{1}{2}\\lambda w^2$, 0.5 because then gradient wrt w is $\\lambda w$.\n",
    "\t- Discourages large weights\n",
    "\t\t- 'tends to improve generalization, because it means that no input dimension can have a very large influence on the scores all by itself'\n",
    "\t\t- encourages more diffuse (vs peaky) vectors\n",
    "\t- During gradient descent param update, each weight is decayed linearly towards zero. `W+= \n",
    "\t-lambda * W`.\n",
    "\t- Aside: Leads to max margin property in SVMs\n",
    "- L1 regularisation\n",
    "\t- $R(W) = \\lambda|w|$\n",
    "\t- Makes weight vectors sparse during optimisation\n",
    "\t\t- Neurons end up using only a sparse subset of most important inputs and become nearly invariant to 'noisy' inputs.\n",
    "- ElasticNet regularisation\n",
    "\t- Combine L1 with L2 regularisation: $R(W) = \\lambda_1|w| + \\lambda_2w^2$\n",
    "- Notes (L1, L2):\n",
    "\t- Claim: L2 expected to give superior performance over L1 if we're not concerend with explicit feature selection (CS231n)\n",
    "\t- Common to regularise only weights W and not biases b because biases don't control the strength of influence of an input dimensioon, but in practice this often turns out to have a negligible effect.\n",
    "\t- If have regularisation loss, cannot achieve zero loss on all examples (assuming examples are distinct), since e.g. for L2 norm, zero loss only possible when W = 0.\n",
    "- Max norm constraints: \n",
    "\t- Absolute upper bound on magnitude of weight vector for each neuron\n",
    "\t- Method: clamp weights after parameter updates\n",
    "\t\t- called projected gradient descent? TODO: check.\n",
    "\t- Pros: Network cannot 'explode' even when learning rate is set too high because updates always bounded.\n",
    "- Dropout\n",
    "\t- Only keep a neuron active with probability p (hyperparameter), set it to zero otherwise\n",
    "\t- [Paper by Srivastava et. al., 2014](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n",
    "\t- NOTE for raw implementations: usually scale by 1/p AND drop during train time to keep the expected output of each neuron the same during test and train\n",
    "- DropConnect (not common)\n",
    "\t- Random set of weights set to zero during forward pass\n",
    "- Regularising bias (not common)\n",
    "\t- But rarely leads to significantly worse performance\n",
    "- Parameters like scaling of regularisation loss usually determined by cross-validation\n",
    "\t\n",
    "#### Helper functions in loss functions\n",
    "- Hinge Loss\n",
    "\t- $f(x) = max(0, x)$\n",
    "\t- Squared Hinge Loss: $max(0, -)^2$\n",
    "\t\t- Penalises violated margins more strongly. Less common.\n",
    "\n",
    "### Optimisation\n",
    "- Key hyperparameter: learning rate (step size of gradient descent)\n",
    "- Centered difference formula\n",
    "\t- $[f(x+h)-f(x-h)]/2h$ \n",
    "\t- in practice often better for computing numeric gradient than the typical gradient formula ($[f(x+h)-f(x)]/h$).\n",
    "\t- Twice as expensive but more precise (Error terms on order of $O(h^2))$ (second order approximation) vs typical formula with error on order of $O(h)$ (first order approximation).\n",
    "- Gradient check: computing the analytic gradient and comparing it to the numerical gradient to check the correctness of your (analytical) implementation\n",
    "\t- Consider the relative error $\\frac{|f'_a - f'_n|}{\\max(|f'_a|, |f'_n|)}$\n",
    "\t\t- AND explicitly keep track of the case where both are zero and pass the gradient check in that edge case.\n",
    "\t\t- Empirical figures for relative error (CS231n):\n",
    "\t\t\t- > 1e-2: gradient probably wrong\n",
    "\t\t\t- between (1e-2, 1e-4): uncomfortable\n",
    "\t\t\t- under 1e-4: usually okay for objectives with kinks, but too high otherwise\n",
    "\t\t\t- under 1e-7: good\n",
    "\t\t- Will get higher relative errors with deeper networks\n",
    "\t- Use double precision\n",
    "\t- More at [CS231n](http://cs231n.github.io/neural-networks-3/)\n",
    "- Gradient descent:\n",
    "\t- repeatedly evaluating the gradient and then performing a parameter update (`weights += step_size *  weights_grad`)\n",
    "- Mini-batch gradient descent:\n",
    "\t- Parameter update after computing gradient over a batch (subset) of the training data \n",
    "\t- Works because examples in the training data are correlated\n",
    "- Stochastic gradient descent\n",
    "\t- Mini-batch size = 1. \n",
    "\t- In practice people often call mini-batch gradient descent SGD.\n",
    "<!-- TODO: implement MGD etc -->\n",
    "\n",
    "\n",
    "### Backpropagation\n",
    "- TODO: add implementation examples and links to those examples\n",
    "- Staged computation: breaking up functions into modules for which you can easily derive local gradients, and then chaining them with the chain rule.\n",
    "\t- Group together expressions as a gate (e.g. sigmoid) for convenience \n",
    "- Practical notes\n",
    "    - Useful to cache forward pass variables\n",
    "- Multiply gate:\n",
    "\t- (two arguments) Local gradients are input values switched then multiplied by output gradient\n",
    "\t- So scale of gradients is directly proportional to scale of inputs.\n",
    "\t- So preprocessing matters: if input too large, need to lower learning rate to compensate.\n",
    "\n",
    "### Parameter updates\n",
    "- Using gradients computing via propagation to perform parameter updates\n",
    "- SGD\n",
    "\t- Vanilla update\n",
    "\t\t- Change parameters along negative gradient direction (to minimise loss)\n",
    "\t\t- `x+= - learning_rate * dx`\n",
    "\t- Momentum update\n",
    "\t\t- ```\n",
    "\t\tv = mu * v - learning_rate * dx # integrate velocity\n",
    "\t\tx += v # integrate position\n",
    "\t\t```\n",
    "\t\t\t- 'Momentum' parameter mu is more like the coefficient of friction.\n",
    "\t\t\t\t- Damps velocity and reduces kinetic energy of system, else particle would never come to a stop at the bottom of a hill.\n",
    "\t\t\t\t- Usually set to [0.5, 0.9, 0.95, 0.99] when cross-validated.\n",
    "\t\t\t- Optimisation can sometimes benefit a little from momentum schedules, where momentum is increased in later stages of learning.\n",
    "\t\t\t\t- E.g. init 0.5, anneal to 0.99 over multiple epochs.\n",
    "\t\t- 'Parameter vector will build up velocity in any direction than has consistent gradient' <!-- TODO: ? -->\n",
    " \t\t- Physics perspective:\n",
    "\t\t\t- Loss as height of hilly terrain, proportional to potential energy U = mgh\n",
    "\t\t\t- Initialising parameters with random numbers seen as equivalent to setting a particle with zero initial velocity at some location\n",
    "\t\t\t- Optimisation process equivalent to process of simulating parameter vector (particle) as rolling on the landscape\n",
    "\t\t\t- Force on particle $F = -\\grad U$, so force felt by particle is negative gradient of loss.\n",
    "\t\t\t- F = ma, so negative gradient is proportional to acceleration of particle.\n",
    "\t\t- VS SGD: Gradient only directly influences the velocity, which in turn has an effect on the position. VS SGD gradient directly integrates the position.\n",
    "\t- Nesterov momentum update\n",
    "\t\t- ```\n",
    "\t\tv_prev = v\n",
    "\t\tv = mu + v - learning_rate * dx # velocity update same\n",
    "\t\tx += -mu * v_prev + (1+mu) * v # position update changes form\n",
    "\t\t```\n",
    "\t\t- Idea: Compute gradient at lookahead position `x + mu * v` instead of old position `x`, since momentum alone pushes particle to lookahead position.\n",
    "\t\t- Stronger theoretical convergence guarantees for convex functions\n",
    "\t\t- In practice consistently works slightly better than standard momentum\n",
    "- Second order methods (not common in practice)\n",
    "\t- $x \\leftarrow x - [Hf(x)]^{-1}\\grad f(x)$.\n",
    "\t\t- $Hf(x)$ being the Hessian matrix, a square matrix of second-order partial derivatives of the function (describes local curvature of loss fn)\n",
    "\t\t- No learning rate hyperparameters\n",
    "\t\t- Impractical for most DL applications because computing Hessian is costly in space and time\n",
    "\t\t    - So people have developed quasi-Newnton methods that approximate the inverted Hessian, e.g. L-BFGS.\n",
    "\t    - But naive L-BFGS must be computed over entire training set. Getting L-BFGS to work on mini-batches is tricky.\n",
    "\n",
    "### Annealing learning rate\n",
    "- Learning rate decay\n",
    "\t- In practice find step decay slightly preferable because hyperparameters are more interpretable.\n",
    "\t- Better to err on the side of slower decay and train for longer if you can afford the computational budget\n",
    "\t- Step decay: reduce lr by some factor (e.g. 0.5) every k epochs\n",
    "\t\t- Heuristic: reduce lr by a constant whenever the validation error stops improving\n",
    "\t- Exponential decay `lr = init_lr*exp(-k*t)`\n",
    "\t\t- init_lr, k: hyperparameters\n",
    "\t\t- t: iteration number (or epoch)\n",
    "\t- 1/t decay `lr = init_lr/(1+k*t)`\n",
    "\t\t- init_lr, k: hyperparameters\n",
    "\t\t- t: iteration number\n",
    "\n",
    "#### Methods to tune learning rate\n",
    "- Adagrad\n",
    "\t```\n",
    "\t# Assume the gradient dx and parameter vector x\n",
    "\tcache += dx**2\n",
    "\tx += - learning_rate * dx / (np.sqrt(cache) + eps)\n",
    "\t``` <!-- TODO: what is cache's init value? -->\n",
    "\t\t- cache: used to normalise parameter update step element-wise. Weights with higher gradients have effective lr reduced and vice versa\n",
    "\t\t- eps: smoothing term (1e-8 to 1e-4) that avoids division by zero\n",
    "\t- Cons: monotonic learning rate usually proves too aggressive and stops learning too early\n",
    "\t- By Duchi et. al.\n",
    "- RMSprop\n",
    "\t``` cache = decay_rate * cache + (1 - decay_rate) * dx**2\n",
    "\tx += - learning_rate * dx / (np.sqrt(cache) + eps) # same as Adagrad\n",
    "\t```\n",
    "\t\t- Adjusted Adagrad (to reduce aggressive, monotonically decreasing learning rate -> has equalizing effect but upds do not get monotonically smaller) \n",
    "\t\t\t- Uses moving average of squared gradients instead of squared gradients.\n",
    "\t\t- `decay_rate`: hyperparameter, typical values [0.9, 0.99, 0.999]\n",
    "\t- Very effective\n",
    "- Adam\n",
    "\t```\n",
    "\t# Simplified version\n",
    "\tm = beta1*m + (1-beta1)*dx\n",
    "\tv = beta2*v + (1-beta2)*(dx**2)\n",
    "\tx += - learning_rate * m / (np.sqrt(v) + eps)\n",
    "\t```\n",
    "\t- Smooth version of gradient (m) used rather than raw, possibly noisy gradient vector `dx`.\n",
    "\t- Full version includes bias correction:\n",
    "\t\t- Corrects for fact that in the first few timesteps m, v both initialised and therefore biased at zero)\n",
    "\t\t\t```\n",
    "\t\t\t# t is your iteration counter going from 1 to infinity\n",
    "\t\t\tm = beta1*m + (1-beta1)*dx\n",
    "\t\t\tmt = m / (1-beta1**t)\n",
    "\t\t\tv = beta2*v + (1-beta2)*(dx**2)\n",
    "\t\t\tvt = v / (1-beta2**t)\n",
    "\t\t\tx += - learning_rate * mt / (np.sqrt(vt) + eps)\n",
    "\t\t\t```\n",
    "\t- Like RMSprop with momentum\n",
    "\t- Currently recommended as default algorithm to use\n",
    "\n",
    "### Activation functions\n",
    "- In practice, usually use ReLU. Be careful with learning rates, possibly monitor fraction of 'dead' units in a network. Don't use sigmoid.\n",
    "- ReLU $f(x)=\\max(0,x)$\n",
    "\t- Pros:\n",
    "\t\t- Greatly accelerates the converges of SGD compared to the sigmoid/tanh functions, possibly due to linear, non-saturating form (Krizhevsky et. al.).\n",
    "\t\t- Cheaper computationally than tanh, sigmoid\n",
    "\t- Cons:\n",
    "\t\t- ReLU units can irreversibly 'die' during training: a large gradient flowing through a ReLU neuron could cause weights to update such that the neuron will never activate on any datapoint again.\n",
    "\t\t\t- so gradient through that unit will always be zero.\n",
    "\t\t\t- Tackle by e.g. **decreasing learning rate**.\n",
    "- Leaky ReLU: $f(x) = 1(x<0)(\\alpha x) + 1(\\geq 0)(x)$, where $\\alpha$ is a small constant.\n",
    "\t- Attempt to fix 'dying ReLU' problem. When x < 0, function has small negative slope instead of being zero.\n",
    "\t<!-- TODO: verify through differentiating -->\n",
    "\t- Consistency unclear\n",
    "- Sigmoid $\\sigma(x) = \\frac{1}{1+e^{-x}}$\n",
    "\t- $\\frac{d\\sigma(x)}{dx} = (1-\\sigma(x))\\sigma(x)$\n",
    "\t- Rarely used now because\n",
    "\t\t- Sigmoids saturate and kill gradients: when neuron's activation saturates at 0 or 1, the gradient at these regions is almost zero\n",
    "\t\t\t- So almost no signal will flow through the neuron to its weights and thus to its data\n",
    "\t\t\t- Also need to be careful that initial weights don't saturate at 0 or 1 \n",
    "\t\t- Sigmoid outputs are not zero-centered (less serious)\n",
    "\t\t\t- -> neurons in later layers would be receiving data that is not zero-centered\n",
    "\t\t\t- If data coming into neurons is always positive, gradients on weights during backprop will either always be positive or always be negative.\n",
    "\t\t\t- May introduce zigzagging dynamics in gradient updates\n",
    "\t\t\t- BUT once gradients summed across a batch, 'the final update for the weights can have variable signs, somewhat mitigating this issue' <!-- TODO: =? Also add code examples for these two points -->\n",
    "- Tanh\n",
    "\t- Squashes real number to range [-1,1]\n",
    "\t- Activations saturate, but output is zero-centered. \n",
    "\t\t- So tanh always preferred to sigmoid.\n",
    "\t\t- Tanh is scaled sigmoid neuron: $\\tanh(x) = 2\\sigma(2x) - 1$\n",
    "- Maxout $\\max(w_1^Tx+b_1, w_2^Tx+b_2)$\n",
    "\t- Generalisation of ReLU and Leaky ReLU\n",
    "\t- Pros & Cons\n",
    "\t\t- Has benefits of ReLU (linear, no saturation) without drawbacks (dying ReLU).\n",
    "\t\t- BUT doubles number of parameters for each neuron, leading to a high total number of paramaters\n",
    "\t- Introduced by Goodfellow et. al.\n",
    "\n",
    "### Before learning (sanity checks)\n",
    "- Check you're getting the loss you expect when initialising with small parameters\n",
    "\t- Check data loss alone\n",
    "\t\t- e.g. CIFAR-10 with Softmax classifier expect initial loss to be -ln(0.1) = 2.302\n",
    "\t\t- e.g. Weston Watkins SVM expect all desired margins to be violated, so expected loss = 9\n",
    "\t- Then increasing regularisation strength should increase loss\n",
    "- Overfit a tiny subset of data\n",
    "\t- e.g. 20 examples, make sure you can achieve zero cost\n",
    "\t- Set regularisation = 0 for this.\n",
    "\n",
    "### During learning: things to monitor\n",
    "- Loss against epochs:\n",
    "\t- if it goes flat (or increases quickly), learning rate likely too high\n",
    "\t- if linearly decreasing, learning rate likely too low\n",
    "\t- Variation between epochs increases as batch size decreases\n",
    "- Training and validation accuracy\n",
    "- Ratio of magnitudes of updates:weights (magnitudes)\n",
    "\t- update_scale/weights_scale should be around 1e-3 (heuristic)\n",
    "\t\t- magnitude = norm of vector\n",
    "\t\t- If lower, learning rate might be too low and vice versa \n",
    "\t- alt: keep track of norm of gradients and gradient updates, usually correlated and give approximately the same results\n",
    "- Activation / Gradient distributions per layer\n",
    "\t- Diagnosing incorrect initialisation\n",
    "\t\t- Can identify all neurons outputting zero or all neurons being completely saturated at either -1 or 1.\n",
    "\t- Method: Plot activation / gradient histograms: should not see strange distributions\n",
    "- (For images) First-layer visualisations\n",
    "\n",
    "### Key hyperparameters to tune\n",
    "- Initial learning rate\n",
    "- Learning rate decay schedule (e.g. decay constant)\n",
    "- Regularisation strength (L2 penalty, dropout strength)\n",
    "\n",
    "### Tips for hyperparameter optimisation\n",
    "- One validation fold (of respectable size) vs CV simplifies the code base\n",
    "- Search for hyperparameters on log scale. E.g. `learning rate = 10 ** uniform(-6,1)`. Similar for regularisation strength, since these have multiplicative effects on the training dynamics.\n",
    "\t- vs dropout usually searched in original scale\n",
    "- Prefer random search to grid search ([Bergstra and Bengio](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf))\n",
    "\t- Often the case that some hyperparameters matter much more than others\n",
    "- Check best values are not on order (else more optimal settings may be outside your interval)\n",
    "- First search coarse search (and with fewer epochs, e.g. 1 epoch), then move to finer search\n",
    "\t- Some hyperparameter settings can lead the model to not learn at all, or immediately explode with infinite cost\n",
    "- Bayesian Hyperparameter Optimisation: \n",
    "\t- Algorithms to more efficiently navigate the hyperparameter space via exploration-exploitation tradeoff\n",
    "\t- e.g. Spearmint, SMAC, Hyperopt. \n",
    "\t- In practice (for CNNs) it's hard to beat random search in carefully-chosen intervals\n",
    "\n",
    "### Ensembles\n",
    "- In practice training independent models and averaging predictions at test time is a reliable approach to improve performance of NNs\n",
    "- Approaches to forming an ensemble\n",
    "\t- Same model, different initialisations (with hyperparameters determined by CV)\n",
    "\t- Top few hyperparameter configurations\n",
    "\t- Different checkpoints of a single model\n",
    "\t- Running (exponentially decaying) average of weights used during training\n",
    "\t\t- Intuition: Objective is bowl-shaped, network is jumping around mode, so average has higher chance of being somewhere near the mode.\n",
    "\n",
    "### Convolutional Neural Networks\n",
    "- Properties\n",
    "\t- Constrain architecture assuming input consists of images (or similarly structured data)\n",
    "\t\t- inputs and outputs are 3D volumes (width, height, depth of image)\n",
    "        - Motivation: regular neural nets don't scale well to full images\n",
    "    - Shared weights: reduce number of parameters\n",
    "- Convolutional Layer\n",
    "\t- Filters: Each filter is smaller than image along width, height dimensions, depth same as image.\n",
    "\t- Operation: Convolve (// slide) filter across width and height of input and compute dot products between entries of filter and input at every position\n",
    "\t- Output: Produce 2D activation map for each depth layer -> stack them to produce output\n",
    "\t- Connectivity\n",
    "\t\t- Locally (vs fully) connected: connect each neuron to only a local region of the input volume. \n",
    "\t\t\t- (i.e. Sparse connectivity: only connect each neuron to a subset of neurons in the previous layer)\n",
    "\t\t- Parameter: receptive field of neuron (filter size/dimensions)\n",
    "\t- Spatial arrangement\n",
    "\t\t- Depth of output volume (Number of filters)\n",
    "\t\t\t- Each filter is meant to learn to look for something different in the input, e.g. presence of various oriented edges or blobs of colour.\n",
    "\t\t\t- Depth column / fibre: Set of neurons all looking at the same region of the input\n",
    "\t\t- Stride: Number of pixels we move at a time when we slide the filter around the image\n",
    "\t\t\t- Usually 1 or 2\n",
    "\t\t\t- Larger stride produces smaller output spatially (width, height)\n",
    "\t\t- Padding\n",
    "\t\t\t- Zero padding: pad input volume with zeros around the border.\n",
    "\t\t\t\t- To control size of output volume, usually to keep it the same dim as the input volume\n",
    "\t\t\t\t- Hyperparameter: size of zero padding\n",
    "\t\t- Output spatial size (along each of width, height): (W-F+2P)/S + 1, where\n",
    "\t\t\t- W = input vol size\n",
    "\t\t\t- F = filter size\n",
    "\t\t\t- S = stride\n",
    "\t\t\t\t- If (W-F+2P) is not divisible by S -> depends how yo uhandle it, e.g. can floor (crop image) or throw error\n",
    "\t\t\t- P = padding (on one side)\n",
    "\t\t- Weight sharing\n",
    "\t\t\t- Weights for filter are the same within one neuron.\n",
    "\t\t\t\t- so forward pass can be computed as a convolution of neuron's weights with input volume\n",
    "\t\t\t- Assume that 'if one feature is useful to compute at some spatial position (x1,y1), then it should also be useful to compute at a different position (x2,y2).'\n",
    "\t\t\t\t- Locally-connected layer: relax parameter sharing scheme since it may not make sense if e.g. input images to ConvNet have specific centered structure\n",
    "\t\t- im2col implementation (matrix multiplication)\n",
    "\t\t\t- Method:\n",
    "\t\t\t\t- Matrix `X_col`: convert each filter-sized region into a column vector.\n",
    "\t\t\t\t- Matrix `W_row`: convert each filter (depth slice) into a row vector\n",
    "\t\t\t\t- Multiply `X_col` with `W_row` (e.g. `np.dot(X_col, W_row)`) and reshape it to proper output dimensions\n",
    "\t\t\t- Cons: Memory-intensive (some values replicated in `X_col`)\n",
    "\t\t\t- Pros: There are very efficient implementations of matrix multiplication we can use (e.g. BLAS API)\n",
    "\t\t- Backpropagation: Also a convolution. TODO: derive\n",
    "\t\t- 1x1 convolution\n",
    "\t\t\t- Makes sense because we operate over 3D and filters always extend through full depth of input volume (TODO: link and elaborate)\n",
    "\t\t- Dilated convolutions\n",
    "\t\t\t- Spaces in between each cell in the filter\n",
    "\t\t\t- Allows you to merge spatial information across inputs more aggressively since receptive field grows much quicker\n",
    "- Pooling Layer\n",
    "\t- Resizes (downsamples) each depth slice of input using MAX operation (Max-pooling)\n",
    "\t\t- Other less common pooling operations: Average pooling or L2-norm pooling\n",
    "\t- Motivation: 'To reduce amount of parameters and computation in the network, and hence to also control overfitting'\n",
    "\t- Method\n",
    "\t\t- Hyperparameters: \n",
    "\t\t\t- spatial extent F (filter width, height)\n",
    "\t\t\t- stride S\n",
    "\t\t- Output of size W2 = (W1 - F)/S + 1 (same for height)\n",
    "\t\t\t- where W1 is input width, W2 is output width\n",
    "\t\t- Common parameterisation: 2x2 filters, stride of 2 (discards 75% of activations)\n",
    "\t- Introduces zero parameters\n",
    "\t- Backpropagation: \n",
    "\t- Looking ahead:\n",
    "\t\t- [Discarding pooling (2015)](https://arxiv.org/abs/1412.6806) \n",
    "\t\t- Discarding pooling layers has been found to be important in training good generative models like variational autoencoders and genarative adversarial networks.\n",
    "- Normalisation Layer\n",
    "\t- Fallen out of favour because in practice their contribution has been shown to be minimal\n",
    "- Converting Fully connected layer to Conv layer\n",
    "\t- TODO: add, see [cs231n](http://cs231n.github.io/convolutional-networks/)\n",
    "- Common ConvNet architectures\n",
    "\t- INPUT -> [[CONV -> RELU]*N -> POOL?]*M -> [FC -> RELU]*K -> FC\n",
    "\t\t- where usually N <= 3, K < 3. N, M, K all non-negative.\n",
    "\t\t- Two CONV layers before every POOL layer 'generally a good idea for larger and deeper networks, because multiple stacked CONV layers can develop more complex features of the input volume before the deestructive pooling operation'\n",
    "- Heuristics\n",
    "\t- Prefer a stack of small filter CONVs to one large receptive field CONV layer (that have identical spatial extent).\n",
    "\t\t- Stack of small filters contain non-linearities that make features more expressive\n",
    "\t\t- Uses fewer parameters\n",
    "\t\t- BUT might need more memory to hold intermediate CONV layer results for backprop\n",
    "\t- CNN Layer Sizing Patterns\n",
    "\t\t- Input layer: multiples of powers of 2 (e.g. 32 (CIFAR-10), 64, 96 (STL-10), 224 (ImageNet), 384, 512)\n",
    "\t\t- Conv layers: \n",
    "\t\t\t- small filters (3x3 to 5x5)\n",
    "\t\t\t- stride = 1\n",
    "\t\t\t\t- smaller strides work better in practice and allow us to leave all down-sampling to POOL layers\n",
    "\t\t\t\t- may have to compromise (increase stride) due to memory constraints, usually compromise only input layer.\n",
    "\t\t\t- pad with zeros such that spatial dimensions of input remains the same\n",
    "\t\t\t\t- padding also improves performance, else info at borders 'washed away' too quickly.\n",
    "\t\t- Pool layers\n",
    "\t\t\t- Most common: max-pooling with 2x2 receptive fields, stride=2 (discards 75% of activations)\n",
    "\t\t\t- rare to see F > 3 because then pooling 'is then too lossy and aggressive and usually leads to worse performance'.\n",
    "- Looking ahead\n",
    "\t- Paradigm of linear list of layers has recently been challenged in Google's Inception architectures and Residual Networks from Microsoft Research Asia\n",
    "\t- Hinton's Capsule Networks\n",
    "- In practice\n",
    "\t- Use whatever works best on ImageNet. Download a pretrained model and finetune it on your data.\n",
    "- Famous ConvNet architectures\n",
    "\t- LeNet\n",
    "\t- AlexNet (ImageNet 2012 winner)\n",
    "\t- ZF Net (Zeiler and Fergus, ILSVRC 2013 winner)\n",
    "\t- GoogLeNet (Szegedy et. al., ILSVRC 2014 winner)\n",
    "\t\t- Developed 'Inception Module' that dramatically reduces parameters in the network (4M from AlexNet's 60M)\n",
    "\t\t- Uses average pooling instead of fully connected layers at the top of the ConvNet\n",
    "\t\t\t- Eliminates a large number of parameters that don't seem to matter much\n",
    "\t\t- Followed up by Inception-v4 etc.\n",
    "\t- VGGNet (Simonyan and Zisserman, ILSVRC 2014 runner-up)\n",
    "\t\t- Showed that the depth of the network is a critical component for good performance.\n",
    "\t\t\t- 16 layers\n",
    "\t\t- Homogenous architecture that only performs 3x3 convs and 2x2 pooling from beginning to end.\n",
    "\t\t- Cons: More expensive to evaluate, uses more memory and parameters (140M params, mostly in first FC layer)\n",
    "\t\t\t- since found that first FC layers can be removed with no performance downgrade, greatly reducing number of parameters (first FC layer has 100M parameters)\n",
    "\t- ResNet (Residual Network, Kaiming He et. al., winner of ILSVRC 2015)\n",
    "\t\t- Skip connections\n",
    "\t\t- Heavy use of batch normalisation\n",
    "\t\t- No fully connected laters at the end\n",
    "\t\t- State-of-the-art as of May 10, 2016\n",
    "- Computational Considerations\n",
    "\t- Memory bottleneck\n",
    "\t\t- Many modern GPUs have limits of 3/4/6 GB memory\n",
    "\t\t- Memory sources:\n",
    "\t\t\t- Activations\n",
    "\t\t\t\t- Raw activations (forward pass values) and gradients at each layer of a convnet. Kept because they are needed for backpropgation\n",
    "\t\t\t\t\t- (TODO: could in principle reduce this...? see [cs231n page](http://cs231n.github.io/convolutional-networks/))\n",
    "\t\t\t- Parameters\n",
    "\t\t\t\t- Network parameters, their gradients during backpropagation, commonly also a step cache if optimisation is using momentum, Adagrad or RMSprop.\n",
    "\t\t\t\t- Estimate: num params x 3\n",
    "\t\t\t- Misc. memory\n",
    "\t\t\t\t- e.g. image data batches and possibly their augmenetd verisons\n",
    "\t\t- Estimating memory: number of values x 4 (bypes) / 1024**3 (GB).\n",
    "\t\t- Decrease batch size to make things fit (since most memory usually consumed by activations)\n",
    "- Visualising what CNNs learn\n",
    "\t- Visualising activations and first-layer weights\n",
    "\t\t- (ReLU activations) As training progresses (deeper layers) activations usually become more sparse and localised\n",
    "\t\t- Dead filters (possible symptoms of high learning rates) easily spotted when activation maps are all zero for many different inputs\n",
    "\t- Weights\n",
    "\t\t- Usually most interpretable on first Conv layer which is looking directly at the raw pixel data\n",
    "\t\t- 'Noisy patterns can be an indicator of a network that hasn't been trained for long enough, or possibly a very low regularisation strength that may have led to overfitting.'\n",
    "\t\t\t- well-trained nets usually display nice and smooth filters without noisy patterns\n",
    "\t- Images that maximally activate a neuron\n",
    "\t\t- But ReLU neurons don't necessarily have any semantic meaning individually: more likely that multiple ReLU neurons are basis vectors of some space\n",
    "\t- Embedding vectors right before classifier (including the ReLU non-linearity)\n",
    "\t\t- e.g. in t-SNE to get a 2D vector for each image -> see which images are close to each other\n",
    "\t- Occluding (hiding) parts of the image\n",
    "\t\t- Plot probability of correct class as a function of the position of an occluder object \n",
    "\t\t\t- plot as a heat map\n",
    "\n",
    "### Transfer Learning\n",
    "- Common to use a pretrained CNN as an initialisation or fixed feature extractor for task of interest (vs training CNN from scratch)\n",
    "\t- Pretrain a CNN on a large dataset (e.g. ImageNet 1.2M images with 1k categories)\n",
    "- Fixed feature extractor:\n",
    "\t- Remove last FC layer (which outputs 1k class scores for ImageNet, say)\n",
    "\t- Treat rest of the network as a feature extractor. These features are called *CNN codes*.\n",
    "\t\t- e.g. AlexNet 4096D vectors.\n",
    "\t\t- Need to be ReLU'd if they were also RELU'd during pretraining.\n",
    "\t- Train linear classifier for the new datase on these features.\n",
    "- Fine-tuning pretrained CNN\n",
    "\t- Can only fine-tune later layers of CNN (motivation: earlier layers should contain more generic features, later layers more specific.)\n",
    "\t- Rules of thumb:\n",
    "\t\t- New dataset small\n",
    "\t\t\t\t- Small data -> Not good idea to fine-tune CNN due to overfitting concerns. Best to train only linear classifier.\n",
    "\t\t\t\t- New dataset similar to original dataset: \n",
    "\t\t\t\t    - Similar: Expect high-level features in CNN to be relevant to this dataset\n",
    "\t\t\t\t    - Approach: Train linear classifier on CNN codes\n",
    "\t\t\t    - New dataset very different from original dataset:\n",
    "\t\t\t    \t- Train classifier from activations earlier in the network\n",
    "\t\t- New dataset large\n",
    "\t\t\t- Can have more confidence we won't overfit if we try to fine-tune the entire network\n",
    "\t\t\t- New dataset similar to original dataset:\n",
    "\t\t\t\t- Finetune entire network\n",
    "\t\t\t- New dataset v different from original dataset\n",
    "\t\t\t\t- Finetune entire network.\n",
    "\t\t\t\t- Can consider training CNN from scratch, but usually don't. \n",
    "- Other practical advice\n",
    "\t- If you use pretrained models, you may be constrained by their architecture choices.\n",
    "\t\t- But can run pretrained net on images of different spatial size due to parameter sharing (adjust strides)\n",
    "\t- Learning rates\n",
    "\t\t- Common to use smaller lr for CNN weights that are being fine-tuned because we expect initial CNN weights to be good.\n",
    "\n",
    "### Recurrent Neural Networks\n",
    "\n",
    "\n",
    "### Other techniques\n",
    "- Softmax\n",
    "\t- Binary logistic regression generalised to multiple classes\n",
    "\t- Outputs normalised class probabilities\n",
    "\t- TODO: where should I put this? Loss? 'final-step classifier'?\n",
    "- t-SNE (t-Distributed Stochastic Neighbour Embedding)\n",
    "\t- Dimensionality reduction technique for visualisation of high-dimensional datasets\n",
    "\t- Gives each datapoint a location in a 2D or 3D map.\n",
    "\t- Method:\n",
    "\t\t- Converts Euclidean distances between datapoints into conditional probabilities that represent similarities\n",
    "\t\t- TODO: complete\n",
    "\t- [[Website]](http://lvdmaaten.github.io/tsne/) [[Paper]](http://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf)\n",
    "\n",
    "### Other CS231n notes (Temporary)\n",
    "- Claim: Many image tasks such as object detection and segmentation can be reduced to image classification\n",
    "- Linear Classifiers\n",
    "\t- Linear classifier Wx+b is effectively running K (number of classes) classifiers at the same time.\n",
    "\t- Interpreting linear classifiers as template matching: each row of W corresponds to a template/prototype for one of the classes. Score of classes obtained by comparing each template with image using inner product (dot product)\n",
    "\t\t- Distance: Negative inner product\n",
    "- Bias trick (to represent W, b as one)\n",
    "\t- Extend input vector x_i by one additional dimension than holds constant 1 (default bias dimension)\n",
    "\t- W, b merged into new W.\n",
    "- SVM vs Softmax classifiers\n",
    "\t- SVM interprets scores as class scores, loss function encourages correct class to have a score higher by a margin than other class scores\n",
    "\t- Softmax interprets scores as unnormalised log probabilities for each class, encourages log probability of correct class to be high.\n",
    "\t- SVM and Softmax losses are thus not comparable\n",
    "\t- SVM more 'local': happy if score for correct class is sufficiently higher than other classes, vs softmax never fully satisfied\n",
    "\n",
    "### Other terms\n",
    "- Subgradient\n",
    "\n",
    "### References\n",
    "- [CS231n](http://cs231n.github.io/)\n",
    "\n",
    "### Other resources\n",
    "- [Machine Learning for Humans: plain-English explanations of ML](https://medium.com/machine-learning-for-humans/why-machine-learning-matters-6164faf1df12)\n",
    "\t- Caveat: I haven't read this yet\n",
    "- [Jeff Dean's Lecture for YC AI](https://blog.ycombinator.com/jeff-deans-lecture-for-yc-ai/)\n",
    "- [Heroes of Deep Learning interview series by Andrew Ng](https://www.youtube.com/watch?v=-eyhCTvrEtE&list=PLfsVAYSMwsksjfpy8P2t_I52mugGeA5gR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
