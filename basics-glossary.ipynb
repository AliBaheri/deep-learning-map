{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics Glossary\n",
    "\n",
    "\n",
    "### Artificial Neural Networks\n",
    "- Typical Components of a Neural Network\n",
    "\t- Input\n",
    "\t- Score function: maps raw data to class scores (for classification)\n",
    "\t\t- Layers\n",
    "\t\t- Conversion to output\n",
    "\t- Loss Function (also cost function, objective)\n",
    "\t\t- e.g. accuracy?\n",
    "\t- Optimiser (to minimise loss)\n",
    "\n",
    "### Convolutional Neural Networks\n",
    "\n",
    "\n",
    "### Recurrent Neural Networks\n",
    "\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "\n",
    "### Measures of distance\n",
    "\n",
    "Often used for regularisation.\n",
    "\n",
    "- L1 distance \n",
    "\t- $d_1(x, y) = \\sum_i|x_i - y_i|$\n",
    "- L2 distance \n",
    "\t- $d_2(x, y) = \\sqrt{\\sum_i (x_i - y_i)^2}$\n",
    "\n",
    "\n",
    "### Data\n",
    "- Training set\n",
    "\t- Data used to directly train your model\n",
    "- Validation set\n",
    "\t- Data used to tune hyperparameters\n",
    "\t\t- Usually choose hyperparameters with lowest validation error\n",
    "\t- Like a fake test set\n",
    "- Test set\n",
    "\t- Data used to measure generalisation of your model: used only once!\n",
    "- Generalisation\n",
    "- K-fold Cross-validation\n",
    "\t- Iterate over k different validation sets and average performance across them\n",
    "\t- Especially useful if training set is small (e.g. < 1000 exampless).\n",
    "\t- BUT computationally expensive\n",
    "\t- Usually K = 3, 5 or 10\n",
    "\n",
    "### Model performance\n",
    "- Underfit\n",
    "- Overfit\n",
    "- Capacity\n",
    "- Hyperparameters\n",
    "\n",
    "### Things to consider\n",
    "- Computational cost at train and test\n",
    "\n",
    "### Data preprocessing\n",
    "- Centering data by subtracting mean from each feature\n",
    "- Scaling input features so values range from [-1,1]\n",
    "\n",
    "### Loss Functions\n",
    "- Multiclass Support Vector Machine (SVM) loss\n",
    "\t- Wants correct class to have score higher than incorrect classes by at least some fixed margin $\\Delta$.\n",
    "\t- $L_i = \\sum_{j\\ne y_i}\\max(0,s_j-(s_{y_i}-\\Delta))$ \n",
    "\t\t- where $s_j$ is the score for the jth class,\n",
    "\t\t- i is the index for examples\n",
    "\t- Aside: can be formulated in other ways (OVA, AVA)\n",
    "- Cross-entropy loss\n",
    "\n",
    "#### Regularisation\n",
    "- Loss is usually = Data loss + Regularisation loss\n",
    "\t- Regularisation loss is usually NOT a function of the data, but of the model parameters\n",
    "- Regularisation: Preference for certain sets of weights over others. \n",
    "\t- Usually to prevent overfitting or reduce ambiguity when there exist multiple solutions (e.g. when weights $\\lambda W$ all yield same output, for positive real $\\lambda$).\n",
    "- L2 norm\n",
    "\t- Discourages large weights\n",
    "\t\t- 'tends to improve generalization, because it means that no input dimension can have a very large influence on the scores all by itself'\n",
    "\t- Elementwise quadratic penalty over all parameters:\n",
    "\t\t- $R(W) = \\sum_k \\sum_l W_{k,l}^2$\n",
    "\t\t\t- sums up all squared elements of W\n",
    "\t- Aside: Leads to max margin property in SVMs\n",
    "- Parameters like scaling of regularisation loss usually determined by cross-validation\n",
    "- Common to regularise only weights W and not biases b because biases don't control the strength of influence of an input dimensioon, but in practice this often turns out to have a negligible effect.\n",
    "- If have regularisation loss, cannot achieve zero loss on all examples (assuming examples are distinct), since e.g. for L2 norm, zero loss only possible when W = 0.\n",
    "\n",
    "#### Helper functions in loss functions\n",
    "- Hinge Loss\n",
    "\t- $f(x) = max(0, x)$\n",
    "\t- Squared Hinge Loss: $max(0, -)^2$\n",
    "\t\t- Penalises violated margins more strongly. Less common.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other techniques\n",
    "- Softmax\n",
    "\t- Binary logistic regression generalised to multiple classes\n",
    "\t- Outputs normalised class probabilities\n",
    "\t- TODO: where should I put this? Loss? 'final-step classifier'?\n",
    "- t-SNE (t-Distributed Stochastic Neighbour Embedding)\n",
    "\t- Dimensionality reduction technique for visualisation of high-dimensional datasets\n",
    "\t- Gives each datapoint a location in a 2D or 3D map.\n",
    "\t- Method:\n",
    "\t\t- Converts Euclidean distances between datapoints into conditional probabilities that represent similarities\n",
    "\t\t- TODO: complete\n",
    "\t- [[Website]](http://lvdmaaten.github.io/tsne/) [[Paper]](http://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf)\n",
    "\n",
    "### Other CS231n notes (Temporary)\n",
    "- Claim: Many image tasks such as object detection and segmentation can be reduced to image classification\n",
    "- Linear Classifiers\n",
    "\t- Linear classifier Wx+b is effectively running K (number of classes) classifiers at the same time.\n",
    "\t- Interpreting linear classifiers as template matching: each row of W corresponds to a template/prototype for one of the classes. Score of classes obtained by comparing each template with image using inner product (dot product)\n",
    "\t\t- Distance: Negative inner product\n",
    "- Bias trick (to represent W, b as one)\n",
    "\t- Extend input vector x_i by one additional dimension than holds constant 1 (default bias dimension)\n",
    "\t- W, b merged into new W.\n",
    "\n",
    "### Other terms\n",
    "- Subgradient\n",
    "\n",
    "### References\n",
    "- [CS231n](http://cs231n.github.io/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
